#!/usr/bin/env python3
"""
çµæ´»çš„è§†é¢‘è¿›åº¦æ¡å·¥å…· - æ ¸å¿ƒæ¨¡å—

æ”¯æŒä»»æ„GIFè§’è‰²ï¼Œå¯è°ƒæ•´æ‰€æœ‰è§†è§‰å‚æ•°
ä½œè€…: AI Assistant
ç‰ˆæœ¬: 1.0.0
"""

import cv2
import numpy as np
import os
import math
import json
from typing import Tuple, Optional, List, Dict, Any
from PIL import Image, ImageSequence
import tempfile

# ç”¨äºèµ„æºæ–‡ä»¶è·¯å¾„å¤„ç†
from importlib.resources import files

# moviepyä½œä¸ºå¯é€‰ä¾èµ–
try:
    from moviepy import VideoFileClip, ImageSequenceClip
    MOVIEPY_AVAILABLE = True
except ImportError:
    MOVIEPY_AVAILABLE = False
    print("âš ï¸  moviepyæœªå®‰è£…ï¼Œå°†ä½¿ç”¨OpenCVæ¨¡å¼ï¼ˆæ— éŸ³é¢‘ï¼‰")

# æŠ‘åˆ¶OpenCVçš„FFmpegè­¦å‘Šä¿¡æ¯
os.environ['OPENCV_FFMPEG_CAPTURE_OPTIONS'] = 'protocol_whitelist;file,rtp,udp'


class VideoProgressBar:
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        """
        çµæ´»çš„è¿›åº¦æ¡é…ç½®
        
        Args:
            config: é…ç½®å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¯è°ƒæ•´çš„å‚æ•°ã€‚å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤é…ç½®ã€‚
        """
        # å¦‚æœæ²¡æœ‰æä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é…ç½®
        if config is None:
            config = {}
            
        # è¿›åº¦æ¡åŸºæœ¬é…ç½®
        self.bar_height = config.get('bar_height', 40)
        self.bar_color = tuple(config.get('bar_color', [0, 255, 255]))  # BGRæ ¼å¼
        self.background_color = tuple(config.get('background_color', [50, 50, 50]))
        self.position = config.get('position', 'bottom')  # top/bottom
        self.margin = config.get('margin', 25)
        
        # è§’è‰²é…ç½® - ä½¿ç”¨åŒ…å†…èµ„æºçš„ç›¸å¯¹è·¯å¾„
        default_character = self._get_default_character_path()
        self.character_path = config.get('character_path', default_character)
        self.character_size = tuple(config.get('character_size', [60, 60]))
        self.character_offset_x = config.get('character_offset_x', 0)  # è§’è‰²Xè½´åç§»
        self.character_offset_y = config.get('character_offset_y', -5)  # è§’è‰²Yè½´åç§»
        
        # åŠ¨ç”»é…ç½®
        self.enable_bounce = config.get('enable_bounce', True)
        self.bounce_amplitude = config.get('bounce_amplitude', 8)
        self.bounce_speed = config.get('bounce_speed', 0.2)
        self.animation_speed = config.get('animation_speed', 3)  # GIFå¸§åˆ‡æ¢é€Ÿåº¦
        
        # ç‰¹æ•ˆé…ç½®
        self.enable_lightning = config.get('enable_lightning', True)
        self.lightning_chance = config.get('lightning_chance', 0.3)  # ç”µå…‰è§¦å‘æ¦‚ç‡
        self.lightning_color = tuple(config.get('lightning_color', [0, 255, 255]))
        
        self.enable_particles = config.get('enable_particles', True)
        self.particle_color = tuple(config.get('particle_color', [0, 255, 255]))
        self.particle_lifetime = config.get('particle_lifetime', 60)
        
        # æ–‡å­—é…ç½®
        self.text_color = tuple(config.get('text_color', [0, 255, 255]))
        self.text_size = config.get('text_size', 0.8)
        self.text_position = config.get('text_position', 'right')  # left/right/center
        self.text_offset_x = config.get('text_offset_x', 15)
        self.text_offset_y = config.get('text_offset_y', 0)
        
        # è¿›åº¦æ¡æ ·å¼é…ç½®
        self.border_thickness = config.get('border_thickness', 3)
        self.border_color = tuple(config.get('border_color', [255, 255, 255]))
        self.gradient_enabled = config.get('gradient_enabled', True)
        self.glow_enabled = config.get('glow_enabled', True)
        
        # å†…éƒ¨å˜é‡
        self.frame_count = 0
        self.character_frames = self._load_character()
        self.total_frames = len(self.character_frames)
        self.lightning_particles = []
        self.trail_particles = []

    def _get_default_character_path(self) -> str:
        """è·å–é»˜è®¤è§’è‰²è·¯å¾„ (ä½¿ç”¨ importlib.resources)"""
        try:
            # ç›´æ¥ä»'video_progress_pkg'åŒ…ä¸­å¯»æ‰¾èµ„æºæ–‡ä»¶è·¯å¾„
            # è¿™æ˜¯æœ€å¥å£®çš„æ–¹å¼ï¼Œæ— è®ºåŒ…å¦‚ä½•å®‰è£…éƒ½èƒ½å·¥ä½œ
            resource_path = files('video_progress_pkg').joinpath(
                'assets', 'characters', 'pikaqiu.gif')
            return str(resource_path)
        except (ImportError, AttributeError, FileNotFoundError):
            # å¦‚æœä¸Šé¢çš„æ–¹æ³•å› æŸç§åŸå› å¤±è´¥ï¼Œæä¾›ä¸€ä¸ªæœ€ç»ˆçš„å¤‡ç”¨æ–¹æ¡ˆ
            print("âš ï¸ æ— æ³•é€šè¿‡ importlib.resources å®šä½èµ„æºæ–‡ä»¶ï¼Œå°†ä½¿ç”¨ç›¸å¯¹è·¯å¾„ã€‚")
            return 'assets/characters/pikaqiu.gif'

    def _load_character(self) -> List[Tuple[np.ndarray, np.ndarray]]:
        """åŠ è½½è§’è‰²GIFçš„æ‰€æœ‰å¸§ï¼ŒåŒ…å«é€æ˜é®ç½©"""
        frames = []
        
        if not os.path.exists(self.character_path):
            print(f"âŒ è§’è‰²æ–‡ä»¶ä¸å­˜åœ¨: {self.character_path}")
            return [(self._create_default_character(), np.ones(self.character_size[::-1], dtype=np.uint8) * 255)]
        
        try:
            with Image.open(self.character_path) as gif:
                print(f"ğŸ® åŠ è½½è§’è‰²GIF: {gif.size}, å¸§æ•°: {gif.n_frames}")
                
                for frame_idx in range(gif.n_frames):
                    gif.seek(frame_idx)
                    frame_rgba = gif.convert('RGBA')
                    frame_resized = frame_rgba.resize(self.character_size, Image.Resampling.LANCZOS)
                    frame_array = np.array(frame_resized)
                    
                    # åˆ†ç¦»RGBå’ŒAlphaé€šé“
                    rgb_channels = frame_array[:, :, :3]
                    alpha_channel = frame_array[:, :, 3]
                    
                    # è½¬æ¢é¢œè‰²ç©ºé—´ RGB -> BGR
                    frame_bgr = cv2.cvtColor(rgb_channels, cv2.COLOR_RGB2BGR)
                    frames.append((frame_bgr, alpha_channel))
                    
        except Exception as e:
            print(f"âŒ åŠ è½½è§’è‰²GIFå‡ºé”™: {e}")
            default_char = self._create_default_character()
            default_mask = np.ones(self.character_size[::-1], dtype=np.uint8) * 255
            frames = [(default_char, default_mask)]
            
        return frames

    def _create_default_character(self) -> np.ndarray:
        """åˆ›å»ºé»˜è®¤è§’è‰²"""
        size = self.character_size[0]
        char = np.zeros((self.character_size[1], self.character_size[0], 3), dtype=np.uint8)
        
        center = (size // 2, self.character_size[1] // 2)
        radius = min(size, self.character_size[1]) // 3
        
        # ç»˜åˆ¶ç®€å•çš„åœ†å½¢è§’è‰²
        cv2.circle(char, center, radius, self.bar_color, -1)
        cv2.circle(char, (center[0] - radius//3, center[1] - radius//3), 3, (0, 0, 0), -1)
        cv2.circle(char, (center[0] + radius//3, center[1] - radius//3), 3, (0, 0, 0), -1)
        
        return char

    def _blend_with_alpha(self, background: np.ndarray, foreground: np.ndarray, 
                         alpha_mask: np.ndarray, x: int, y: int) -> np.ndarray:
        """ä½¿ç”¨Alphaé€šé“æ··åˆå‰æ™¯å’ŒèƒŒæ™¯"""
        result = background.copy()
        h, w = foreground.shape[:2]
        
        if (y >= 0 and y + h <= background.shape[0] and 
            x >= 0 and x + w <= background.shape[1]):
            
            bg_region = background[y:y+h, x:x+w]
            alpha = alpha_mask.astype(float) / 255.0
            alpha = np.expand_dims(alpha, axis=2)
            
            blended = foreground * alpha + bg_region * (1 - alpha)
            result[y:y+h, x:x+w] = blended.astype(np.uint8)
        
        return result

    def _update_lightning_effects(self, char_x: int, char_y: int):
        """æ›´æ–°ç”µå…‰ç‰¹æ•ˆ"""
        if not self.enable_lightning:
            return
            
        if self.frame_count % 15 == 0 and np.random.random() > (1 - self.lightning_chance):
            lightning = {
                'x': char_x + np.random.randint(-20, 20),
                'y': char_y + np.random.randint(-10, 10),
                'life': 10,
                'intensity': np.random.randint(5, 15)
            }
            self.lightning_particles.append(lightning)
        
        self.lightning_particles = [p for p in self.lightning_particles if p['life'] > 0]
        for particle in self.lightning_particles:
            particle['life'] -= 1

    def _draw_lightning_effects(self, frame: np.ndarray):
        """ç»˜åˆ¶ç”µå…‰ç‰¹æ•ˆ"""
        for particle in self.lightning_particles:
            alpha = particle['life'] / 10.0
            
            start_point = (particle['x'], particle['y'])
            for i in range(3):
                end_x = particle['x'] + np.random.randint(-15, 15)
                end_y = particle['y'] + np.random.randint(-15, 15)
                end_point = (end_x, end_y)
                
                cv2.line(frame, start_point, end_point, self.lightning_color, 2)
                cv2.line(frame, start_point, end_point, (255, 255, 255), 1)

    def _update_trail_particles(self, char_x: int, char_y: int):
        """æ›´æ–°å°¾è¿¹ç²’å­"""
        if not self.enable_particles:
            return
            
        if self.frame_count % 5 == 0:
            particle = {
                'x': char_x + self.character_size[0] // 2 + np.random.randint(-5, 5),
                'y': char_y + self.character_size[1] // 2 + np.random.randint(-5, 5),
                'life': self.particle_lifetime,
                'drift_x': np.random.randint(-2, 1),
                'drift_y': np.random.randint(-2, 2),
                'size': np.random.randint(2, 5)
            }
            self.trail_particles.append(particle)
        
        self.trail_particles = [p for p in self.trail_particles if p['life'] > 0]
        for particle in self.trail_particles:
            particle['life'] -= 1
            particle['x'] += particle['drift_x']
            particle['y'] += particle['drift_y']

    def _draw_trail_particles(self, frame: np.ndarray):
        """ç»˜åˆ¶å°¾è¿¹ç²’å­"""
        for particle in self.trail_particles:
            alpha = particle['life'] / self.particle_lifetime
            color_intensity = int(255 * alpha)
            
            color = tuple(int(c * alpha) for c in self.particle_color)
            cv2.circle(frame, (int(particle['x']), int(particle['y'])), 
                      particle['size'], color, -1)

    def _draw_progress_bar(self, frame: np.ndarray, progress: float, width: int, height: int) -> np.ndarray:
        """ç»˜åˆ¶çµæ´»é…ç½®çš„è¿›åº¦æ¡"""
        frame_copy = frame.copy()
        self.frame_count += 1
        
        # è®¡ç®—è¿›åº¦æ¡ä½ç½®
        bar_width = width - 2 * self.margin
        
        if self.position == "bottom":
            bar_y = height - self.margin - self.bar_height
        else:
            bar_y = self.margin
        
        bar_x = self.margin
        
        # ç»˜åˆ¶è¾¹æ¡†
        if self.border_thickness > 0:
            cv2.rectangle(frame_copy, 
                         (bar_x - self.border_thickness, bar_y - self.border_thickness), 
                         (bar_x + bar_width + self.border_thickness, bar_y + self.bar_height + self.border_thickness),
                         self.border_color, self.border_thickness)
        
        # ç»˜åˆ¶è¿›åº¦æ¡èƒŒæ™¯
        cv2.rectangle(frame_copy, (bar_x, bar_y), 
                     (bar_x + bar_width, bar_y + self.bar_height),
                     self.background_color, -1)
        
        # ç»˜åˆ¶è¿›åº¦
        progress_width = int(bar_width * progress)
        if progress_width > 0:
            if self.gradient_enabled:
                # æ¸å˜è¿›åº¦æ¡
                for i in range(progress_width):
                    ratio = i / progress_width if progress_width > 0 else 0
                    color = tuple(int(c * (0.6 + 0.4 * ratio)) for c in self.bar_color)
                    cv2.line(frame_copy, (bar_x + i, bar_y), 
                            (bar_x + i, bar_y + self.bar_height), color, 1)
            else:
                # çº¯è‰²è¿›åº¦æ¡
                cv2.rectangle(frame_copy, (bar_x, bar_y),
                             (bar_x + progress_width, bar_y + self.bar_height),
                             self.bar_color, -1)
            
            # å‘å…‰æ•ˆæœ
            if self.glow_enabled and self.frame_count % 20 < 10:
                shine_x = bar_x + progress_width - 20
                if shine_x > bar_x:
                    cv2.line(frame_copy, (shine_x, bar_y), 
                            (shine_x, bar_y + self.bar_height), (255, 255, 255), 3)
        
        # è®¡ç®—è§’è‰²ä½ç½®
        base_char_x = bar_x + max(0, progress_width - self.character_size[0]) + self.character_offset_x
        base_char_y = bar_y + self.character_offset_y
        
        if self.position == "top":
            base_char_y = bar_y + self.bar_height + abs(self.character_offset_y)
        else:
            base_char_y = bar_y - self.character_size[1] + self.character_offset_y
        
        # è¾¹ç•Œæ£€æŸ¥
        char_x = max(0, min(base_char_x, width - self.character_size[0]))
        char_y = max(0, min(base_char_y, height - self.character_size[1]))
        
        # å¼¹è·³åŠ¨ç”»
        if self.enable_bounce:
            bounce = int(self.bounce_amplitude * abs(math.sin(self.frame_count * self.bounce_speed)))
            char_y -= bounce
        
        # æ›´æ–°ç‰¹æ•ˆ
        self._update_lightning_effects(char_x, char_y)
        self._update_trail_particles(char_x, char_y)
        
        # ç»˜åˆ¶ç‰¹æ•ˆ
        self._draw_lightning_effects(frame_copy)
        self._draw_trail_particles(frame_copy)
        
        # ç»˜åˆ¶è§’è‰²
        if self.total_frames > 0:
            current_frame_idx = (self.frame_count // self.animation_speed) % self.total_frames
            current_char, current_alpha = self.character_frames[current_frame_idx]
            
            try:
                frame_copy = self._blend_with_alpha(frame_copy, current_char, current_alpha, 
                                                  char_x, char_y)
            except Exception as e:
                print(f"ç»˜åˆ¶è§’è‰²æ—¶å‡ºé”™: {e}")
        
        # ç»˜åˆ¶è¿›åº¦æ–‡å­—
        progress_text = f"{progress*100:.1f}%"
        text_size_info = cv2.getTextSize(progress_text, cv2.FONT_HERSHEY_SIMPLEX, 
                                        self.text_size, 2)[0]
        
        # è®¡ç®—æ–‡å­—ä½ç½® - è·Ÿéšè¿›åº¦æ¡ç§»åŠ¨
        if self.text_position == 'follow':
            # æ–‡å­—è·Ÿéšè¿›åº¦æ¡å‰ç«¯
            progress_end_x = bar_x + progress_width
            text_x = max(bar_x, min(progress_end_x - text_size_info[0] // 2, 
                                   bar_x + bar_width - text_size_info[0]))
            text_x += self.text_offset_x
        elif self.text_position == 'left':
            text_x = bar_x + self.text_offset_x
        elif self.text_position == 'center':
            text_x = bar_x + (bar_width - text_size_info[0]) // 2 + self.text_offset_x
        else:  # right
            text_x = bar_x + bar_width - text_size_info[0] - self.text_offset_x
        
        text_y = bar_y + (self.bar_height + text_size_info[1]) // 2 + self.text_offset_y
        
        # æ–‡å­—é˜´å½±
        for offset in [(2, 2), (-2, 2), (2, -2), (-2, -2)]:
            cv2.putText(frame_copy, progress_text, 
                       (text_x + offset[0], text_y + offset[1]),
                       cv2.FONT_HERSHEY_SIMPLEX, self.text_size, (0, 0, 0), 3)
        
        # ä¸»æ–‡å­—
        cv2.putText(frame_copy, progress_text, (text_x, text_y),
                   cv2.FONT_HERSHEY_SIMPLEX, self.text_size, self.text_color, 2)
        
        return frame_copy

    def process_video(self, input_video: str, output_video: Optional[str] = None) -> str:
        """
        å¤„ç†è§†é¢‘ï¼Œæ·»åŠ è¿›åº¦æ¡å¹¶ä¿ç•™éŸ³é¢‘
        
        Args:
            input_video: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„
            output_video: è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼Œé»˜è®¤åœ¨è¾“å…¥æ–‡ä»¶åŒç›®å½•ç”Ÿæˆï¼‰
            
        Returns:
            str: è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„
            
        Raises:
            FileNotFoundError: è¾“å…¥è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨
            ValueError: è§†é¢‘å¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™
        """
        if not os.path.exists(input_video):
            raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {input_video}")
        
        # æ”¹è¿›è¾“å‡ºè·¯å¾„å¤„ç†
        if output_video is None:
            # åœ¨åŸè§†é¢‘åŒç›®å½•ç”Ÿæˆï¼Œé¿å…ç¡¬ç¼–ç è·¯å¾„
            input_dir = os.path.dirname(input_video)
            if not input_dir:  # å¦‚æœè¾“å…¥è§†é¢‘åœ¨å½“å‰ç›®å½•
                input_dir = "."
            name, ext = os.path.splitext(os.path.basename(input_video))
            output_video = os.path.join(input_dir, f"{name}_progress{ext}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_video)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨moviepy
        if not MOVIEPY_AVAILABLE:
            print("ğŸ“º ä½¿ç”¨OpenCVæ¨¡å¼ï¼ˆæ— éŸ³é¢‘ï¼‰")
            return self._process_video_opencv_fallback(input_video, output_video)
        
        try:
            # ä½¿ç”¨moviepyåŠ è½½è§†é¢‘å¹¶ä¿ç•™éŸ³é¢‘
            print("ğŸ¬ æ­£åœ¨åŠ è½½è§†é¢‘ï¼ˆåŒ…å«éŸ³é¢‘ï¼‰...")
            video_clip = VideoFileClip(input_video)
            
            fps = video_clip.fps
            width, height = video_clip.size
            duration = video_clip.duration
            total_frames = int(fps * duration)
            
            print(f"ğŸ“Š è§†é¢‘ä¿¡æ¯:")
            print(f"  åˆ†è¾¨ç‡: {width}x{height}")
            print(f"  å¸§ç‡: {fps:.1f} FPS")
            print(f"  æ—¶é•¿: {duration:.2f} ç§’")
            print(f"  æ€»å¸§æ•°: {total_frames}")
            print(f"  è§’è‰²åŠ¨ç”»å¸§æ•°: {self.total_frames}")
            print(f"  éŸ³é¢‘: {'âœ… åŒ…å«' if video_clip.audio else 'âŒ æ— éŸ³é¢‘'}")
            
            print("ğŸ® æ­£åœ¨å¤„ç†è§†é¢‘å¸§...")
            
            def process_frame(get_frame, t):
                """å¤„ç†å•ä¸ªå¸§çš„å‡½æ•°"""
                frame = get_frame(t)
                # MoviePy 2.x ç›´æ¥è¿”å›uint8æ ¼å¼(0-255)ï¼Œæ— éœ€ä¹˜ä»¥255
                if frame.dtype == np.float64 or frame.dtype == np.float32:
                    # å¦‚æœæ˜¯æµ®ç‚¹æ•°(0-1)ï¼Œè½¬æ¢ä¸ºuint8
                    frame_bgr = cv2.cvtColor((frame * 255).astype(np.uint8), cv2.COLOR_RGB2BGR)
                else:
                    # å¦‚æœå·²ç»æ˜¯uint8(0-255)ï¼Œç›´æ¥è½¬æ¢
                    frame_bgr = cv2.cvtColor(frame.astype(np.uint8), cv2.COLOR_RGB2BGR)
                
                # è®¡ç®—è¿›åº¦
                progress = t / duration if duration > 0 else 0
                
                # æ·»åŠ è¿›åº¦æ¡
                frame_with_progress = self._draw_progress_bar(frame_bgr, progress, width, height)
                
                # è½¬æ¢å›RGBç»™moviepyï¼ŒMoviePy 2.x æœŸæœ›uint8æ ¼å¼
                frame_rgb = cv2.cvtColor(frame_with_progress, cv2.COLOR_BGR2RGB)
                return frame_rgb.astype(np.uint8)
            
            # åˆ›å»ºæ–°çš„è§†é¢‘å‰ªè¾‘ï¼Œåº”ç”¨è¿›åº¦æ¡å¤„ç† (MoviePy 2.x API)
            processed_clip = video_clip.transform(process_frame)
            
            # ä¿ç•™åŸå§‹éŸ³é¢‘ (MoviePy 2.x API)
            if video_clip.audio:
                processed_clip = processed_clip.with_audio(video_clip.audio)
                print("ğŸ”Š ä¿ç•™åŸå§‹éŸ³é¢‘")
            
            print("ğŸ’¾ æ­£åœ¨å†™å…¥æœ€ç»ˆè§†é¢‘...")
            
            # å†™å…¥è§†é¢‘ï¼Œå¹¶æŒ‡å®šé«˜è´¨é‡å‚æ•° (MoviePy 2.x API)
            processed_clip.write_videofile(
                output_video,
                fps=fps,
                codec='libx264',      # ä½¿ç”¨é«˜è´¨é‡å’Œå…¼å®¹æ€§å¥½çš„H.264ç¼–ç å™¨
                bitrate='10000k',     # è®¾ç½®ä¸€ä¸ªè¾ƒé«˜çš„ç ç‡ (ä¾‹å¦‚ 10000 kbps)ã€‚åŸè§†é¢‘ç ç‡è¶Šé«˜ï¼Œè¿™é‡Œå¯ä»¥è®¾å¾—è¶Šé«˜ã€‚
                preset='medium',      # 'slow'æˆ–'veryslow'å¯ä»¥è·å¾—æ›´é«˜å‹ç¼©ç‡ï¼ˆåŒç­‰ç ç‡ä¸‹è´¨é‡æ›´å¥½ï¼‰ï¼Œä½†è€—æ—¶æ›´é•¿ã€‚'medium'æ˜¯å¾ˆå¥½çš„å¹³è¡¡ç‚¹ã€‚
                threads=4,            # ä½¿ç”¨å¤šä¸ªCPUæ ¸å¿ƒæ¥åŠ é€Ÿç¼–ç 
                logger=None           # MoviePy 2.x: ä½¿ç”¨loggerä»£æ›¿verboseå‚æ•°
            )
            
            # æ¸…ç†èµ„æº
            processed_clip.close()
            video_clip.close()
            
            print(f"ğŸ‰ è§†é¢‘å¤„ç†å®Œæˆ! è¾“å‡ºæ–‡ä»¶: {output_video}")
            
        except Exception as e:
            print(f"âŒ ä½¿ç”¨moviepyå¤„ç†å¤±è´¥: {e}")
            print("ğŸ”„ é™çº§åˆ°OpenCVå¤„ç†ï¼ˆæ— éŸ³é¢‘ï¼‰...")
            return self._process_video_opencv_fallback(input_video, output_video)
        
        return output_video
    
    def _process_video_opencv_fallback(self, input_video: str, output_video: str) -> str:
        """OpenCVé™çº§å¤„ç†ï¼ˆæ— éŸ³é¢‘ï¼Œç”¨ä½œå¤‡é€‰ï¼‰"""
        cap = cv2.VideoCapture(input_video)
        if not cap.isOpened():
            raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {input_video}")
        
        fps = int(cap.get(cv2.CAP_PROP_FPS))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        print(f"âš ï¸  é™çº§æ¨¡å¼ - æ— éŸ³é¢‘è¾“å‡º")
        
        # ç®€åŒ–çš„ç¼–ç å™¨é€‰æ‹©
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))
        
        if not out.isOpened():
            raise ValueError("æ— æ³•åˆ›å»ºè¾“å‡ºè§†é¢‘æ–‡ä»¶")
        
        frame_count = 0
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            progress = frame_count / total_frames
            frame_with_progress = self._draw_progress_bar(frame, progress, width, height)
            out.write(frame_with_progress)
            
            frame_count += 1
            
            if frame_count % (fps * 2) == 0:
                print(f"ğŸ“Š å¤„ç†è¿›åº¦: {progress*100:.1f}% ({frame_count}/{total_frames})")
        
        cap.release()
        out.release()
        return output_video


def load_config(config_path: str) -> Dict[str, Any]:
    """ä»JSONæ–‡ä»¶åŠ è½½é…ç½®"""
    if os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {}


def save_default_config(config_path: str):
    """ä¿å­˜é»˜è®¤é…ç½®åˆ°JSONæ–‡ä»¶"""
    default_config = {
        "input_video": "assets/samples/sample_video.mp4",
        "output_video": "",
        
        "bar_height": 40,
        "bar_color": [0, 255, 255],
        "background_color": [50, 50, 50],
        "position": "bottom",
        "margin": 25,
        
        "character_path": "assets/characters/pikaqiu.gif",
        "character_size": [60, 60],
        "character_offset_x": 0,
        "character_offset_y": -5,
        
        "enable_bounce": True,
        "bounce_amplitude": 8,
        "bounce_speed": 0.2,
        "animation_speed": 3,
        
        "enable_lightning": True,
        "lightning_chance": 0.3,
        "lightning_color": [0, 255, 255],
        
        "enable_particles": True,
        "particle_color": [0, 255, 255],
        "particle_lifetime": 60,
        
        "text_color": [0, 255, 255],
        "text_size": 0.8,
        "text_position": "follow",
        "text_offset_x": 0,
        "text_offset_y": -10,
        
        "border_thickness": 3,
        "border_color": [255, 255, 255],
        "gradient_enabled": True,
        "glow_enabled": True
    }
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(default_config, f, indent=2, ensure_ascii=False)
    
    print(f"ğŸ’¾ é»˜è®¤é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
